\documentclass{article}
\usepackage{nips07submit_e,times}
\usepackage[square, numbers, comma, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography
\usepackage{graphicx}
\usepackage{subfigure}
%\documentstyle[nips07submit_09,times]{article}

\newcounter{note}
\newcommand{\note}[1]{\textbf{NOTE \thenote\addtocounter{note}1: #1}}

\newcommand{\years}{\textrm{y}}
\newcommand{\months}{\textrm{m}}

\newcommand{\rmd}{\textrm{d}}

\newcommand{\event}{E}
%\newcommand{\set}[1]{\textsf{#1}}
\newcommand{\set}[1]{#1}
\newcommand{\group}{\set{G}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\uniform}{\mathcal{U}}

\title{Learning charactieristic orders of events}

\author{
Hubert M. Fonteijn \\
Department of Computer Science\\
University College London\\
London, United Kingdom \\
\texttt{h.fonteijn@cs.ucl.ac.uk} \\
\And
Daniel C. Alexander \\
Department of Computer Science \\
University College London \\
London, United Kingdom \\
\texttt{d.alexander@cs.ucl.ac.uk} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\makeanontitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction}
The analysis of time series and sequences of events is an important subject in Machine Learning. There are several classes of time series data. The most common class consists of series of measurements, together with temporal information for each sample. An example of such data is provided by experiments in which the length of children is repeatedly measurement during their development, together with the time of measurement. In ordered time series data, on the other hand, only the sequence of events is recorded. An example of such data is given by data which records in what order undergraduate students take their courses. 

Another important subdivision in time series data is that between longitudinal and cross-sectional data. In longitudinal experiments, the process of interest is sampled completely in each instance of the process. For instance, in the abovementioned length experiment, this would require length measurements within each child from birth to adolescence. Cross-sectional studies, on the other hand, sample the process of interest only once in each subject, and use data from many subjects in different stages of the process. Longitudinal experiments are statistically more powerful, because they separate the inter-subject variability before the onset of the process from the variability in the process itself. However, in many cases, such as large population studies, only cross-sectional studies are feasible.

In longitudinal experiments, the order of events within each subject is always readily available. Recently, several groups have developed algorithms to estimate group representations of such ordering, possibly containing partial orderings \cite{malerba-learning, mannila2000global}. In cross-sectional data, there is incomplete information about the ordering of events within each subject. Typical orderings therefore need to be reconstructed from data pooled over many subjects. This situation becomes even more difficult when the events' timings are vague. An example of such data comes from fossil records. The data from each fossil site can be regarded as a snapshot of the species that were present at some time in the past. The dating of fossil sites is however often very uncertain. Puolamäki et al. \cite{puolamäki2006seriation} develop an algorithm that discovers the most likely order of appearance and disappearance. They however use a considerable amount of prior information about the likely temporal order of fossil sites.

The aim of this paper is to develop an algorithm that can estimate the order of events, given cross-sectional data about the occurrence of these events, but with minimal temporal information. The central object of our algorithm is the precedence matrix. In this matrix, we express the probability that one event preceeds the other. We derive this precedence matrix for several data types. Given this precedence matrix, we now consider two simple models for ordering events:

1. The \emph{sequential events model} imposes a strict ordering on the
   events.  Suppose we have $N$ events $\event_1, \cdots, \event_N$.
   The model orders the events
   $\event_{k(1)}\prec\event_{k(2)}\prec\cdots\prec\event_{k(N)}$,
   where $k(1), \cdots, k(N)$ are a permutation of the integers $1,
   \cdots, N$ and $\event_i\prec\event_j$ denotes that event
   $\event_i$ occurs before $\event_j$.

2. The \emph{sequential groups model} divides the events into $M$
   groups $\group_1, \cdots, \group_M$, where $\bigcup_{i=1}^M\group_i
   = \{\event_j\}_{j=1}^N$ and $\group_i\cap\group_j = \emptyset$ if
   $i\neq j$.  The groups have a strict ordering so that
   $\event_i\prec\event{j}$ if and only if $k<l$ where
   $\event_i\in\group_k$ and $\event_j\in\group_l$.
   
Consider for instance an experiment in which data about the development of children is collected. Here are some events that occur in during most children's development: 1. onset of puberty, 2. adult teeth arrive, 3. first word, 4. first kiss, 5. 13th birthday, 6. first tooth, 7. first day at school.  Given dates of each event for a good number of children, the best fit sequential events model might order these events 6, 3, 7, 2, 1, 5, 4.  This ordering represents a most common ordering among the group of children, although clearly some variance occurs: some children will not hit puberty until after their 13th birthday for example, but most hit puberty before 13.  The sequential groups model is more flexible and allows for events to happen at around the same time, we might find groups $\{3, 6\}, \{2, 7\}, \{1, 4, 5\}$.

We develop MCMC algorithms to perform inference on both models, including model selection to determine the optimal number of groups in the \emph{sequential groups model}. The remainder of this paper is structured as follows: In section \ref{precedence_matrix} we introduce the precedence matrix for different data types. In section \ref{model_fitting} we develop the MCMC algorithm for the Sequeential events model and the Sequential Groups model. In section \ref{experiments} we introduce the data set on which we test our model. Section \ref{results} discusses the results of applying our algorithm to that data and section \ref{conclusion} concludes.

\section{Theory}
\subsection{Precedence matrix}
\label{precedence_matrix}
In this section we develop the precedence matrix. We start from the case of longitudinal data, and then move towards the less trivial cases of cross-sectional data. We then derive the precedence matrix for cross-sectional data in which the measured data is only an indirect indicator of the process of interest. We illustrate all three cases with the abvoementioned example of the experiment measuring childrens' development.

\subsubsection{Longitudinal data}
The most informative data set comes from performing a longitudinal experiment, in which a sample of the children is followed during their complete development. At each sample point the children are asked what events have so far occurred to them. After this data is collected, this then leads to an unambiguous ordering of events in each individual. Suppose we acquire such data from $K$ subjects. A simple estimate of $p(\event_i\prec\event_j)$ comes directly from the proportion of the population for which event $\event_i$ occurred before event $\event_j$ so that 
\begin{equation}\label{drpsum}
p(\event_i\prec\event_j) = \frac{|\set{X}_{i\prec j}|}{K}
\end{equation}
where $\set{X}_{i\prec j}$ is the subset of the population that listed $\event_i$ before $\event_j$ and $|\set{X}|$ is the cardinality of set $\set{X}$.

Now suppose each subject also provides dates for each event.  This information provides a more robust estimate of $p(\event_i\prec\event_j)$ such as 
\begin{equation}\label{drpweighted}
p(\event_i\prec\event_j) = \frac{\sum_{x\in\set{X}_{ij}} (d_{xj} - d_{xi})}{\sum_{x\in\set{X}} |d_{xj} - d_{xi}|}
\end{equation}
where $\set{X}$ is the full set of all $K$ individuals and $d_{xi}$ is the date of event $\event_i$ for individual $x$.  Both estimates are robust to missing data and the second to noisy estimates of the $d_{xi}$.  The second estimate should be more robust with small data sets, but may be more vulnerable to outliers, than the first.

\subsubsection{Direct cross-sectional data}
Let us consider the case of cross-sectional data. Here we get only a snapshot from each subject, who is only part way through the sequence of events (i.e. all events may not have happened in each snapshot).  In our developmental study, this might come from asking a set of children (or their parents) with a range of ages whether each of the events has or has not happened for them yet. This data set would be much easier to acquire than the data set in the previous example.

With such a data set, we might estimate $p(\event_i\prec\event_j)$ by limiting attention only to the set $\set{X}_{i\oplus j}$ for which one or other of the two events has occurred, but not both.  Then we might
estimate
\begin{equation}\label{dspsum}
p(\event_i\prec\event_j) = \frac{|\set{X}_{i\wedge \neg j}|}{|\set{X}_{i\oplus j}|}
\end{equation}
where $\set{X}_{i\wedge \neg j}$ is the set of individuals for which $\event_i$ has occured, but $\event_j$ has not.  In general, the set $\set{X}_{i\otimes j}$ refers to the set of individuals for which $\event_i\otimes \event_j$ is true, where $\otimes$ is some binary operator.  If, in addition, we have date information telling us how long ago each event that has happened occurred, we can incorporate it into the estimate to weight more heavily contributions from individuals showing greater times between the two events:
\begin{equation}\label{dspweighted}
p(\event_i\prec\event_j) = \frac{\sum_{\set{X}_{i\wedge \neg j}} (d_x - d_{xi})}{\sum_{\set{X}_{i\wedge \neg  j}} (d_x - d_{xi}) + \sum_{\set{X}_{j\wedge \neg  i}} (d_x - d_{xj})}
\end{equation}
where $d_x$ is the date of data collection from individual $x$.

In the latter scenario, we can further incorporate information from individuals in the set $\set{X}_{i\wedge j}$ for whom both events $\event_i$ and $\event_j$ have occurred:
\begin{equation}\label{dspinclusive}
p(\event_i\prec\event_j) = \frac{\sum_{\set{X}_{i\wedge \neg  j}} (d_x - d_{xi}) + \sum_{\set{X}_{i\prec j}} (d_{xj} - d_{xi})}{\sum_{\set{X}_{i\wedge \neg  j}} (d_x - d_{xi}) + \sum_{\set{X}_{j\wedge \neg  i}} (d_x - d_{xj}) + \sum_{\set{X}_{i\wedge j}} |d_{xj} - d_{xi}| },
\end{equation}
where $\set{X}_{i\prec j}$ is now the set of individuals for which both events $\event_i$ and $\event_j$ have occurred and $\event_i$ occured before $\event_j$.
Equation~\ref{dspinclusive} takes the simpler form
\begin{equation}
p(\event_i\prec\event_j) = \frac{\sum_{\set{X}_{i\prec j}} (d_{xj} - d_{xi})}{\sum_{\set{X}} |d_{xj} - d_{xi}| },
\end{equation}
if we set $d_{xi}$ equal for all events yet to happen and include $\set{X}_{i\wedge \neg j}$ in $\set{X}_{i\prec j}$.

\note{\emph{The direct snapshot formulae are basically just the direct retrospective ones with missing data.}}

\subsubsection{Indirect cross-sectional data}

Finally, suppose that rather than direct information specifying occurrence of events, we have only measurements of some marker that we believe indicates occurrence of the event.  For example, suppose we
cannot trust the answers that children give us about dates of events. Instead, we choose to use indirect measurements to indicate the likelihood that an event has already happened.  For example, we might
use height measurements to indicate the onset of puberty.

Now we need to construct a mapping that relates height to the probability that puberty has onset. We can use height data from prepubescent children and construct a null distribution for the probability of finding a child with a certain height. If this probability is low enough, we will reject this hypothesis and decide that this child is in her puberty. In this case, we decide that the onset of puberty has happened in this subject, otherwise we decide that the onset of puberty has not happened. This then can be directly used for the calculation of the precedence matrix as in equation \ref{dspsum} and \ref{dspweighted}.

\subsection{Model fitting}
\label{model_fitting}
Given a precedence matrix $P$, we can evaluate the likelihood of candidate models, which provides an objective function for model fitting.  The simplest model fitting problem is to locate the maximum
likelihood model, but we are also interested to sample from the posterior distribution on the model given the data.
\note{Need more motivation for MCMC model?}

For the sequential events model, the likelihood of a candidate ordering $\event_{k(1)}\prec\event_{k(2)}\prec\cdots\prec\event_{k(N)}$ is
\begin{equation}\label{seqmodlik}
L(k) = \prod_{i=1}^{N-1}\left(\prod_{j=i+1}^N p(\event_{k(i)} \prec \event_{k(j)}) \right).
\end{equation}

We use a Markov-Chain Monte-Carlo (MCMC) algorithm to sample from the posterior distribution on the ordering.  The starting point $k_0$ is a random ordering of the events.  A perturbation $k'$ of the current model
swaps the positions of two randomly chosen events.  The next step $k_1 = k'$ with probability $\min(a, 1)$ where $a = L(k')/L(k_0)$ is the likelihood ratio; otherwise $k_1 = k_0$.  Repeating the procedure
provides the MCMC chain.
\note{What about priors?}

For the sequential groups model with $M$ groups, $\group_1 \prec \group_2 \prec \cdots \prec \group_M$, the likelihood of a candidate group assignment is 
\begin{equation}\label{grpmodlik}
L(G_1, \cdots, G_M) = \prod_{i=1}^{M-1} \left( \prod_{j=i+1}^M p(\group_i \prec \group_j) \right)p(G_i),
\end{equation}
where
\begin{equation}
p(\group_i \prec \group_j) = \prod_{k=1}^{|\group_i|} \prod_{l=1}^{|\group_j|} p(\group_{ik} \prec \group_{jl}),
\end{equation}
is the probability that all events $\group_{ik}$, $k = 1, \cdots, |\group_i|$, in $\group_i$ occur before all events in $\group_j$, and 
\begin{equation}
p(\group_i) = \prod_{k=1}^{|\group_i|-1} \prod_{l=k+1}^{|\group_i|} p(\group_{ik}\sim \group_{il})
\end{equation}
is the probability that $\group_i$ is a genuine group, ie all events in $\group_i$ occur at the same time.  If the estimate of $p(\event_1\prec \event_2)$ in $P$ is independent of that of $p(\event_2 \prec
\event_1)$, then a sensible estimate of $p(\group_{ik}\sim\group_{il})$ is 
\begin{equation}
p(\group_{ik}\sim \group_{il}) = 1 - p(\group_{ik}\prec \group_{il}) - p(\group_{il}\prec \group_{ik})
\end{equation}
However, in many definitions of $P$, $p(\event_1 \prec \event_2) = 1 - p(\event_2 \prec \event_1)$, in which case, we might use
\begin{equation}\label{simpleprobsim}
p(\group_{ik}\sim \group_{il}) = (p(\group_{ik}\prec \group_{il})p(\group_{il}\prec \group_{ik}))^\frac12.
\end{equation}
The square root in equation~\ref{simpleprobsim} ensures that the product in equation~\ref{grpmodlik} always contains the same number of elements of $P$ regardless of the grouping so that neither large nor
small groups are unfairly favoured.

We can use a similar MCMC algorithm for sampling the posterior distribution on the group assignment for a fixed $M$.  The starting point is a random assignment of events to groups.  Perturbations move
a randomly chosen event to a different randomly chosen group.

An additional problem with the sequential groups model is to determine the number of groups.  Bayesian model selection offers a potential solution.  We can write the probability of $M$ groups given the data
$D$ as
\begin{equation}\label{grpmodsel}
p(M | D) = \frac{p(D|M)p(M)}{\sum_{i=1}^Y p(D|i)p(i)}
\end{equation}
where we marginalize the group assignment $\hat{\group}$ to obtain
\begin{equation}\label{assmarg}
p(D|i) = \int p(D|i,\hat\group)p(\hat\group)\rmd\hat\group.
\end{equation}
We can evaluate equation~\ref{assmarg} by summing over the MCMC chain. The most likely number of groups is then the $M$ that maximizes $p(M|D)$.  Note that the denominator in equation~\ref{grpmodsel} is
independent of $M$ and it is reasonable to choose $p(M)$ uniform. Thus we simply choose the $M$ that maximizes $p(D|M)$.

The model selection above requires independent estimates of $p(\event_1 \prec \event_2)$ and $p(\event_2 \prec \event_1)$.
Otherwise, if $p(\event_1 \prec \event_2) = 1 - p(\event_2 \prec\event_1)$, we find that
\begin{equation}
p(\event_1 \sim \event_2)^2 = p(\event_1 \prec \event_2)p(\event_2 \prec \event_1) \le \max(p(\event_1 \prec \event_2)^2, p(\event_2 \prec \event_1)^2),
\end{equation}
with equality occuring only when $p(\event_1 \prec\event_2)=p(\event_2 \prec \event_1)=0.5$.  Thus the model selection always favours larger $M$.  For the model selection to work properly,
we need an independent estimate of $p(\event_1 \sim \event_2)$.
\note{Still not clear to me??}

\section{Experiments}
\label{experiments}
This section presents a simple simulation using childhood events as an illustrative example, as in the Methods section.  We estimate distributions of timings for each of the seven events and study the
precedence matrix that we estimate using various data sets.

We have determined the distribution of onsets for the following events:

\begin{enumerate}

\item \emph{Onset of puberty.} The Wikipedia page on ``Puberty'' is a good source of data and references.  The typical age of onset is 10 for girls and 12 for boys \cite{chumlea1982physical}, although the variance of
normal onset is wide and the precise definition is not clear, as several stages occur for which typical onsets are as early as age 7.

This simulation uses the age distribution $\normal(11\years, 1.5\years)$ for event $\event_1$, which we truncate at a minimum age of $6\years$.

\item \emph{Adult teeth arrive.} Adult teeth arrive fairly consistently around the age of 6 and variance appears fairly low from a trawl of child development websites.

The simulation uses the age distribution $\normal(6\years, 6\months)$ for event $\event_2$.

\item \emph{First word.} Opinion of child development website varies somewhat on this, most likely because of the lack of a consistent definition of the first word; does simple repetition of the sound
count, or does it require actual understanding of the meaning?  We'll take the first coherent utterance, which can occur as early as $4\months$, but typically between $10$ and $15\months$.

The simulation uses the age distribution $\normal(13\months, 3\months)$ truncated at a minimum of $4\months$ for event $\event_3$.

\item \emph{First kiss.} Definitions are again a problem, but a trawl of teen-gossip sites suggests a fairly wide distribution with a typical age of around 13.

The simulation uses the age distribution $\normal(13\years, 2\years)$ truncated at a minimum of $5\years$ for event $\event_4$.

\item \emph{13th birthday.} Minimal variance here!

The simulation uses the age distribution $\delta(13\years)$ for event $\event_5$.

\item \emph{First tooth.} Child-development sites suggest a fairly wide variance of the normal range with typical ages of $7\months$.

The simulation uses the age distribution $\normal(7\months, 3\months)$ truncated at a minimum of $1\months$ for event $\event_6$.

\item \emph{First day at school.} In the UK, all children who will be 5 before Augst 31st of the following year start primary school in September.  Thus on their first day, children's ages can range from $4\years$ and one day to exactly $5\years$.  The distribution of birthdays over the year is slightly non-uniform (\verb+www.panix.com/~murphy/bday.html+) but not by much.

The simulation uses the age distribution $\uniform(4\years, 5\years)$ for event $\event_7$.

\end{enumerate}

\section{Results}
\label{results}

\begin{figure}
\includegraphics[width=13cm]{./Code1/GrowingUpDists.png}
\caption{Histogram of 1000 samples of different features in the simulated data of the child development experiment}
\label{GrowingUpDists}
\end{figure}

Figure \ref{GrowingUpDists} shows histograms of 1000 samples drawn from the distributions on each event.  We assume independence of all events throughout this section.

\subsection{Longitudinal data set}

The first data set provides direct retrospective dates for each event from 1000 individuals.  Estimates of the precedence matrix $P$ come directly from equations~\ref{drpsum} and~\ref{drpweighted}.  Figure
\ref{GU_DirRetroPs} shows $P$ estimates from each equation (left and right respectively) both from all 1000 data points and for successively smaller subsets.  The colour scale in the figure is hot to cold with dark red corresponding to a probability of 1 and dark blue to zero.

\begin{figure}
\centering
\subfigure[bladiebla]{
\includegraphics[scale=0.35]{./Code1/GU_DirRetroPs.png}
\label{GU_DirRetroPs}}
\subfigure[bladiebla]{
\includegraphics[scale=0.35]{./Code1/GU_DirSnapshotPs.png}
\label{GU_DirSnapshotPs}}
\end{figure}

We can consider the matrices in the top row as ground truth as $1000$ samples is enough to get a good estimate of each probability.  The large-sample matrices from the two formulae show good consistency.  As
we reduce the number of samples, the estimate from the direct sum in equation \ref{drpsum} departs from the large sample estimate more rapidly than the estimate from the weighted sum in equation \ref{drpweighted}.

\subsubsection{Direct cross-sectional data set}

The second data set contains just snap shots for each individual, which we construct by assigning a random age for each and keeping only dates in the first data set that are less than the age.  Estimates of
$P$ come from equations~\ref{dspsum}, \ref{dspweighted}, and~\ref{dspinclusive}. Figure \ref{GU_DirSnapshotPs} shows $P$ estimates from each equation (left to right respectively) both from all 1000 data points and for successively smaller subsets.  The colour scale is as for the previous figure.

Large numbers of subjects provide good agreement with the retrospective data.  The estimate that includes information about pairs of events that have already happened, equation~\ref{dspinclusive}, provides the most robust estimates as the number of samples reduces.

\note{Include MCMC algorithm}

\section{Discussion and Conclusion}
\label{conclusion}
In this paper, we develop a new algorithm for determining the order of events from cross-sectional data. We introduce the concept of the precedence matrix and show how it can be derived for several data types. Finally, we develop a MCMC algorithm for estimating either the sequential events model or the sequential groups model. In the latter case, we show how the number of groups can be determined using Bayesian model selection. We show good performance of the sequential events model in a simulation of data of children's development.
\note{Well, not yet...}

Puolamäki et al. \cite{puolamäki2006seriation} develop an algorithm for determining at what times species were present from fossil data. In their case, species appear and disappear from the fossil record, which introduces a symmetry in the temporal order at which these species originate and die out. More specifically, if we have evidence that in three different time periods species A was present, species A and B were present and species B was present respectively, this could with equal probability indicate that the order of species origination (and extinction) was AB or BA. Puolamäki et al. therefore need to put strong priors on the temporal order of \emph{some} sites to break this symmetry and recover the temporal order of species origination. We on the other hand assume that each sample contains information about present \emph{and past} events, which allows us to construct the precedence matrix from which we estimate the most likely order.

In MCMC, convergence always needs to be critically evaluated, because if the MCMC chain has not converged, we can obtain biased statistics. Puolamäki et al. for instance need to run numerous chains to assess their convergence. We on the other hand achieve easy convergence within a single chain.
\note{\emph{OK, this is sloppy, should be in the methods section, and maybe a figure to back it up?}}

In our example using simulated data from the develoment of kids, we only expect there to be one canonical order of events, characterizing normal development. There could be however cases in which there is a mixture of typical event orders, such as data in which children with some developmental disease are combined with data from normally developing children. The estimation of these mixtures of event orders will be investigated in future work. Such mixtures can be investigated using the partial order work by Mannila and Meek \cite{mannila2000global}. Their algorithms do however assume knowledge about the complete order of events \emph{within each subject}, which makes their algorithms unsuitable for the application to cross-sectional data.

We expect the algorithm and its extensions to have a wide variety of applications, such as the study of the progress of development and disease progression, the analysis of traffic patterns, software systems analysis and the analysis of industrial processes. In disease progression, we could for instance study in what order disease symptoms and biomarker scores typically occur. This could lead to predictive models of disease progression, which would have significant diagnostic value. 

\label{Bibliography}
\bibliographystyle{unsrtnat}  % Use the "unsrtnat" BibTeX style for formatting the Bibliography
\bibliography{bibliography}  % The references (bibliography) information are stored in the file named "Bibliography.bib"


\end{document}
